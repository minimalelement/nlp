{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Lab 4&5 .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minimalelement/nlp/blob/main/NLP_Lab_4%265_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Write a program to find the antonyms of the words in the document by using Wordnet.**"
      ],
      "metadata": {
        "id": "0MSyAv10Lt7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "# Import necessary modules\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZPI6qJ-GYtO",
        "outputId": "52cf75b5-3ba2-4d11-dc0d-da0e4abdf72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Predefined Sample**"
      ],
      "metadata": {
        "id": "MhzRCXGKNbZ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bzwIXl0GBwa",
        "outputId": "3f992069-2646-4eb3-8ad6-3954b9690262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ferocity', 'fierceness', 'furiousness', 'force', 'fury', 'vehemence', 'violence', 'wildness'}\n",
            "set()\n"
          ]
        }
      ],
      "source": [
        "def synonym_antonym_extractor(phrase):\n",
        "     from nltk.corpus import wordnet\n",
        "     synonyms = []\n",
        "     antonyms = []\n",
        "\n",
        "     for syn in wordnet.synsets(phrase):\n",
        "          for l in syn.lemmas():\n",
        "               synonyms.append(l.name())\n",
        "               if l.antonyms():\n",
        "                    antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "     print(set(synonyms))\n",
        "     print(set(antonyms))\n",
        "\n",
        "synonym_antonym_extractor(phrase=\"violence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Reading user data**"
      ],
      "metadata": {
        "id": "CLtwA3dkNgPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputdata = open('/content/sample_data/football.txt', 'r')\n",
        "data = inputdata.read()\n",
        "\n",
        "\n",
        "print(data)\n",
        "\n",
        "\n",
        "textdata = word_tokenize(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "n1EHwzOIHM_l",
        "outputId": "3a65bd7c-889a-40a1-daf8-c74dc6541b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1a1d9cf6d811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/football.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/football.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textdata"
      ],
      "metadata": {
        "id": "WuTovpb8K-w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "antonyms = []\n",
        "\n",
        "for k in textdata:\n",
        "  for syn in wordnet.synsets(k):\n",
        "      for i in syn.lemmas():\n",
        "          if i.antonyms():\n",
        "                antonyms.append(i.antonyms()[0].name())"
      ],
      "metadata": {
        "id": "f4KlXD_RGNt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_words = set(stopwords.words('english'))\n",
        "  \n",
        "# filtered_sentence = [w for w in textdata if not w.lower() in stop_words]\n",
        "  \n",
        "# filtered_sentence = []\n",
        "  \n",
        "# for w in textdata:\n",
        "#     if w not in stop_words:\n",
        "#         filtered_sentence.append(w)\n",
        "  \n",
        "# print(textdata)\n",
        "\n",
        "# print(filtered_sentence)\n",
        "\n"
      ],
      "metadata": {
        "id": "izWkGpzb3XIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms = []\n",
        "\n",
        "for k in textdata:\n",
        "  for syn in wordnet.synsets(k):\n",
        "      for i in syn.lemmas():\n",
        "          synonyms.append(i.name())\n",
        "\n",
        "# print(set(synonyms))\n",
        "print(\"Antonyms are : \", set(antonyms))\n",
        "print(\"Synonyms are : \", set(synonyms))"
      ],
      "metadata": {
        "id": "X66pstla4fpT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}